import duckdb
import s3fs
import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
import os
from dotenv import load_dotenv

# Config JAVA
os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-17-openjdk-amd64'
os.environ['PATH'] = f"{os.environ['JAVA_HOME']}/bin:" + os.environ['PATH']

# Load credentials
load_dotenv()
aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')
aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')
s3_bucket = os.getenv('AWS_BUCKET_NAME')
s3_prefix = 'process_videos'

def list_all_parquet_files():
    fs = s3fs.S3FileSystem(key=aws_access_key_id, secret=aws_secret_access_key)
    fullpath = f'{s3_bucket}/{s3_prefix}'
    return [f's3a://{f}' for f in fs.ls(fullpath) if f.endswith('.parquet')]

def create_spark_session():
    return SparkSession.builder \
        .appName('Transform Fact Views') \
        .config('spark.jars', '/home/thangtranquoc/jars/hadoop-aws-3.3.4.jar,/home/thangtranquoc/jars/aws-java-sdk-bundle-1.11.1026.jar') \
        .config('spark.hadoop.fs.s3a.access.key', aws_access_key_id) \
        .config('spark.hadoop.fs.s3a.secret.key', aws_secret_access_key) \
        .config('spark.hadoop.fs.s3a.endpoint', 's3.amazonaws.com') \
        .config('spark.hadoop.fs.s3a.impl', 'org.apache.hadoop.fs.s3a.S3AFileSystem') \
        .config('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') \
        .config('spark.hadoop.fs.s3a.path.style.access', 'true') \
        .getOrCreate()

def process_file(file_path, spark, conn):
    print(f"\nüì• ƒêang x·ª≠ l√Ω file: {file_path}")
    df = spark.read.parquet(file_path)

    if df.rdd.isEmpty():
        print(f"‚ö†Ô∏è B·ªè qua file {file_path} v√¨ kh√¥ng c√≥ d·ªØ li·ªáu.")
        return

    # Chuy·ªÉn trending_date t·ª´ "yy.dd.MM" sang DATE h·ª£p l·ªá
    df = df.withColumn('trending_date', F.to_date('trending_date', 'yy.dd.MM'))

    # L·∫•y video_sk t·ª´ DuckDB
    video_lookup = conn.execute("SELECT video_sk, video_id FROM dim_video").df()
    if video_lookup.empty:
        print("‚ùå B·∫£ng dim_video tr·ªëng, kh√¥ng th·ªÉ ti·∫øp t·ª•c.")
        return

    video_lookup_spark = spark.createDataFrame(video_lookup)

    df_joined = df.join(video_lookup_spark, on='video_id', how='inner')

    # Chu·∫©n h√≥a schema + b·ªè NULL
    df_fact = df_joined.select(
        'video_sk', 'trending_date', 'views', 'likes', 'dislikes',
        'comment_count', 'country_code'
    ).dropna(subset=['video_sk', 'trending_date', 'views'])

    # B·ªè tr√πng theo PK
    df_fact = df_fact.dropDuplicates(['video_sk', 'trending_date'])

    # Th·ªëng k√™
    print(f"üßæ T·ªïng s·ªë d√≤ng g·ªëc: {df.count()}")
    print(f"üîÅ Sau join video: {df_joined.count()} d√≤ng")
    print(f"üì¶ Sau khi l·ªçc NULL & duplicate: {df_fact.count()} d√≤ng")

    if df_fact.count() == 0:
        print(f"‚ö†Ô∏è Kh√¥ng c√≥ d√≤ng h·ª£p l·ªá ƒë·ªÉ insert t·ª´ {file_path}")
        return

    # Insert v√†o DuckDB
    conn.register('fact_views_df', df_fact.toPandas())
    conn.execute("""
        INSERT INTO fact_views (
            video_sk, trending_date, views, likes, dislikes, comment_count, country_code
        )
        SELECT * FROM fact_views_df
        ON CONFLICT(video_sk, trending_date) DO NOTHING
    """)
    print("‚úÖ ƒê√£ insert v√†o b·∫£ng fact_views.")

def transform_to_dw_2():
    spark = create_spark_session()
    conn = duckdb.connect('/home/thangtranquoc/youtube-project/youtube-trending-etl/datawarehouse.duckdb')
    parquet_files = list_all_parquet_files()

    for file in parquet_files:
        try:
            process_file(file, spark, conn)
        except Exception as e:
            print(f"‚ùå L·ªói x·ª≠ l√Ω file {file}: {str(e)}")

    conn.close()
    spark.stop()
    print("\nüéâ Ho√†n t·∫•t insert b·∫£ng fact_views.")

# Run
transform_to_dw_2()
